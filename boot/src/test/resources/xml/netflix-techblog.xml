<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Netflix TechBlog - Medium]]></title>
        <description><![CDATA[Learn about Netflix’s world class engineering efforts, company culture, product developments and more. - Medium]]></description>
        <link>https://medium.com/netflix-techblog?feed=rss----2615bd06b42e---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Netflix TechBlog - Medium</title>
            <link>https://medium.com/netflix-techblog?feed=rss----2615bd06b42e---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 06 Apr 2019 00:03:21 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/netflix-techblog" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Building and Scaling Data Lineage at Netflix to Improve Data Infrastructure Reliability, and…]]></title>
            <link>https://medium.com/netflix-techblog/building-and-scaling-data-lineage-at-netflix-to-improve-data-infrastructure-reliability-and-1a52526a7977?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/1a52526a7977</guid>
            <category><![CDATA[big-data]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Mon, 25 Mar 2019 14:01:00 GMT</pubDate>
            <atom:updated>2019-03-25T14:01:00.996Z</atom:updated>
            <content:encoded><![CDATA[<h3>Building and Scaling Data Lineage at Netflix to Improve Data Infrastructure Reliability, and Efficiency</h3><p>By: <a href="https://www.linkedin.com/in/di-lin-b3b37b26/">Di Lin</a>, <a href="https://www.linkedin.com/in/girish-lingappa-309aa24/">Girish Lingappa</a>, <a href="https://www.linkedin.com/in/jitenderaswani/">Jitender Aswani</a></p><p><strong>Imagine </strong>yourself in the role of a data-inspired decision maker staring at a metric on a dashboard about to make a critical business decision but pausing to ask a question — “Can I run a check myself to understand what data is behind this metric?”</p><p>Now, imagine yourself in the role of a software engineer responsible for a micro-service which publishes data consumed by few critical customer facing services (e.g. billing). You are about to make structural changes to the data and want to know who and what downstream to your service will be impacted.</p><p>Finally, imagine yourself in the role of a data platform reliability engineer tasked with providing advanced lead time to data pipeline (ETL) owners by proactively identifying issues upstream to their ETL jobs. You are designing a learning system to forecast Service Level Agreement (SLA) violations and would want to factor in all upstream dependencies and corresponding historical states.</p><p>At Netflix, user stories centered on understanding data dependencies shared above and countless more in Detection &amp; Data Cleansing, Retention &amp; Data Efficiency, Data Integrity, Cost Attribution, and Platform Reliability subject areas inspired Data Engineering and Infrastructure (DEI) team to envision a comprehensive data lineage system and embark on a development journey a few years ago. We adopted the following mission statement to guide our investments:</p><blockquote>“Provide a complete and accurate data lineage system enabling decision-makers to win moments of truth.”</blockquote><p>In the rest of this blog, we will a) touch on the complexity of Netflix cloud landscape, b) discuss lineage design goals, ingestion architecture and the corresponding data model, c) share the challenges we faced and the learnings we picked up along the way, and d) close it out with “what’s next” on this journey.</p><h3>Netflix Data Landscape</h3><p>Freedom &amp; Responsibility (F&amp;R) is the lynchpin of Netflix’s <a href="https://jobs.netflix.com/culture">culture</a> empowering teams to move fast to deliver on innovation and operate with freedom to satisfy their mission. Central engineering teams provide paved paths (secure, vetted and supported options) and guard rails to help reduce variance in choices available for tools and technologies to support the development of scalable technical architectures. Nonetheless, Netflix data landscape (see below) is complex and many teams collaborate effectively for sharing the responsibility of our data system management. Therefore, building a complete and accurate data lineage system to map out all the data-artifacts (including in-motion and at-rest data repositories, Kafka topics, apps, reports and dashboards, interactive and ad-hoc analysis queries, ML and experimentation models) is a monumental task and requires a scalable architecture, robust design, a strong engineering team and above all, amazing cross-functional collaboration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gYI3uCywVhSrcoRo" /><figcaption>Data Landscape</figcaption></figure><h3>Design Goals</h3><p>At the project inception stage, we defined a set of design goals to help guide the architecture and development work for data lineage to deliver a complete, accurate, reliable and scalable lineage system mapping Netflix’s diverse data landscape. Let’s review a few of these principles:</p><ul><li><strong>Ensure data integrity</strong> — Accurately capture the relationship in data from disparate data sources to establish trust with users because without absolute trust lineage data may do more harm than good.</li><li><strong>Enable seamless integration — </strong>Design the system to integrate with a growing list of data tools and platforms including the ones that do not have the built-in meta-data instrumentation to derive data lineage from.</li><li><strong>Design a flexible data model<em> </em></strong>— Represent a wide range of data artifacts and relationships among them using a generic data model to enable a wide variety of business use cases.</li></ul><h3>Ingestion-at-scale</h3><p>The data movement at Netflix does not necessarily follow a single paved path since engineers have the freedom to choose (and the responsibility to manage) the best available data tools and platforms to achieve their business goals. As a result, a single consolidated and centralized feed of truth does not exist that can be leveraged to derive data lineage truth. Therefore, the ingestion approach for data lineage is designed to work with many disparate data sources.</p><p>Our data ingestion approach, in a nutshell, is classified broadly into two buckets — push or pull. Today, we are operating using a pull-heavy model. In this model, we scan system logs and metadata generated by various compute engines to collect corresponding lineage data. For example, we leverage <a href="https://medium.com/netflix-techblog/inviso-visualizing-hadoop-performance-f834175c6df8">inviso</a> to list pig jobs and then <a href="https://medium.com/netflix-techblog/introducing-lipstick-on-a-pache-pig-f17e0a4e0c89">lipstick</a> to fetch tables and columns from these pig scripts. For spark compute engine, we leverage spark plan information and for Snowflake, admin tables capture the same information. In addition, we derive lineage information from scheduled ETL jobs by extracting workflow definitions and runtime metadata using <a href="https://medium.com/netflix-techblog/meson-workflow-orchestration-for-netflix-recommendations-fc932625c1d9">Meson</a> scheduler APIs.</p><p>In the push model paradigm, various platform tools such as the data transportation layer, reporting tools, and Presto will publish lineage events to a set of lineage related Kafka topics, therefore, making data ingestion relatively easy to scale improving scalability for the data lineage system.</p><h3>Data Enrichment</h3><p>The lineage data, when enriched with entity metadata and associated relationships, become more valuable to deliver on a rich set of business cases. We leverage <a href="https://medium.com/netflix-techblog/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520">Metacat</a> data, our internal metadata store and service, to enrich lineage data with additional table metadata. We also leverage metadata from another internal tool, <a href="https://medium.com/netflix-techblog/evolving-the-netflix-data-platform-with-genie-3-598021604dda">Genie</a>, internal job and resource manager, to add job metadata (such as job owner, cluster, scheduler metadata) on lineage data. The ingestions (ETL) pipelines transform enriched datasets to a common data model (design based on a graph structure stored as vertices and edges) to serve lineage use cases. The lineage data along with the enriched information is accessed through many interfaces using SQL against the warehouse and Gremlin and a REST Lineage Service against a graph database populated from the lineage data discussed earlier in this paragraph.</p><h3>Challenges</h3><p>We faced a diverse set of challenges spread across many layers in the system. Netflix’s diverse data landscape made it challenging to capture all the right data and conforming it to a common data model. In addition, the ingestion layer designed to address several ingestions patterns added to operational complexity. Spark is the primary big-data compute engine at Netflix and with pretty much every upgrade in Spark, the spark plan changed as well springing continuous and unexpected surprises for us.</p><p>We defined a generic data model to store lineage information and now conforming the entity and associated relationships from various data sources to this data model. We are loading the lineage data to a graph database to enable seamless integration with a REST data lineage service to address business use cases. To improve data accuracy, we decided to leverage AWS S3 access logs to identify entity relationships not been captured by our traditional ingestion process.</p><p>We are continuing to address the ingestion challenges by adopting a system level instrumentation approach for spark, other compute engines, and data transport tools. We are designing a CRUD layer and exposing it as REST APIs to make it easier for anyone to publish lineage data to our pipelines.</p><p>We are taking a mature and comprehensive data lineage system and now extending its coverage far beyond traditional data warehouse realms with a goal to build universal data lineage to represent all the data artifacts and corresponding relationships. We are tackling a bunch of very interesting known unknowns with exciting initiatives in the field of data catalog and asset inventory. Mapping micro-services interactions, entities from real time infrastructure, and ML infrastructure and other non traditional data stores are few such examples.</p><h3>Lineage Architecture and Data Model</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Xp1KHPFm1R7GZGAI" /><figcaption>Data Flow</figcaption></figure><p>As illustrated in the diagram above, various systems have their own independent data ingestion process in place leading to many different data models that store entities and relationships data at varying granularities. This data needed to be stitched together to accurately and comprehensively describe the Netflix data landscape and required a set of conformance processes before delivering the data for a wider audience.</p><p>During the conformance process, the data collected from different sources is transformed to make sure that all entities in our data flow, such as tables, jobs, reports, etc. are described in a consistent format, and stored in a generic data model for further usage.</p><p>Based on a standard data model at the entity level, we built a generic relationship model that describes the dependencies between any pair of entities. Using this approach, we are able to build a unified data model and the repository to deliver the right leverage to enable multiple use cases such as data discovery, SLA service and Data Efficiency.</p><h3>Current Use Cases</h3><p>Big Data Portal, a visual interface to data management at Netflix, has been the primary consumer of lineage data thus far. Many features benefit from lineage data including ranking of search results, table column usage for downstream jobs, deriving upstream dependencies in workflows, and building visibility of jobs writing to downstream tables.</p><p>Our most recent focus has been on powering (a) a data lineage service (REST based) leveraged by SLA service and (b) the data efficiency (to support data lifecycle management) use cases. SLA service relies on the job dependencies defined in ETL workflows to alert on potential SLA misses. This service also proactively alerts on any potential delays in few critical reports due to any job delays or failures anywhere upstream to it.</p><p>The data efficiency use cases leverages visibility on entities and their relationships to drive cost and attribution gains, auto cleansing of unused data in the warehouse .</p><h3>What’s next?</h3><p>Our journey on extending the value of lineage data to new frontiers has just begun and we have a long way to go in achieving the overarching goal of providing universal data lineage representing all entities and corresponding relationships for all data at Netflix. In the short to medium term, we are planning to onboard more data platforms and leverage graph database and a lineage REST service and GraphQL interface to enable more business use cases including improving developer productivity. We also plan to increase our investment in data detection initiatives by integrating lineage data and detection tools to efficiently scan our data to further improve data hygiene.</p><p>Please share your experience by adding your comments below and stay tuned for more on data lineage at Netflix in the follow up blog posts. .</p><p><strong><em>Credits:</em></strong> We want to extend our sincere thanks to many esteemed colleagues from the data platform (part of Data Engineering and Infrastructure org) team at Netflix who pioneered this topic before us and who continue to extend our thinking on this topic with their valuable insights and are building many useful services on lineage data.</p><p>We will be at <a href="https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/73025">Strata San Francisco on March 27th in room 2001</a> delivering a tech session on this topic, please join us and share your experiences.</p><p><strong>If you would like to be part of our small, impactful, and collaborative team — come </strong><a href="https://jobs.lever.co/netflix/e8f4481e-86e7-4a80-ab6c-ae581bc3a7d1"><strong>join us</strong></a><strong>.</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1a52526a7977" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/building-and-scaling-data-lineage-at-netflix-to-improve-data-infrastructure-reliability-and-1a52526a7977">Building and Scaling Data Lineage at Netflix to Improve Data Infrastructure Reliability, and…</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Netflix Public Bug Bounty, 1 year later]]></title>
            <link>https://medium.com/netflix-techblog/netflix-public-bug-bounty-1-year-later-8f075b49d1cb?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/8f075b49d1cb</guid>
            <category><![CDATA[security]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Thu, 21 Mar 2019 16:01:00 GMT</pubDate>
            <atom:updated>2019-03-21T16:01:00.789Z</atom:updated>
            <content:encoded><![CDATA[<p>by <a href="https://twitter.com/astha_singhal">Astha Singhal</a> (Netflix Application Security)</p><p>As Netflix continues to create entertainment people love, the security team continues to keep our members, partners, and employees secure. The security research community has partnered with us to improve the security of the Netflix service for the past few years through our responsible disclosure and bug bounty programs. A year ago, we launched our public bug bounty <a href="https://medium.com/netflix-techblog/netflixbugbounty-ae3bf4489def">program</a> to strengthen this partnership and enable researchers across the world to more easily participate.</p><p>When we decided to go public with our bug bounty, we revamped our program terms to bring even more targets (including Netflix streaming mobile apps) in scope and set clearer guidelines for researchers that participate in our program. We have always tried to prioritize a good researcher experience in our program to keep the community engaged. For example, we maintain an average triage time of less than 48 hours for issues of all severity. Since the public launch, we have engaged with 657 researchers from around the world. We have collectively rewarded over $100,000 for over 100 valid bugs in that time.</p><p>We wanted to share a few interesting submissions that we have received over the last year:</p><ul><li>We choose to <a href="https://www.youtube.com/watch?v=L1WaMzN4dhY">focus</a> our security resources on applications deployed via our infrastructure paved road to be able to scale our services. The bug bounty has been great at shining a light on the parts of our environment that may not be on the paved road. A researcher found an application that ran on an older Windows server that was deployed in a non-standard way making it difficult for our automated visibility services to detect it. The system had significant issues that we were grateful to hear about so we could retire the system.</li><li>We received a report from a researcher that found a service they didn’t believe should be available on the internet. The initial finding seemed like a low severity issue, but the researcher asked for permission to continue to explore the endpoint to look for more significant issues. We love it when researchers reach out to coordinate testing on an issue. We looked at the endpoint and determined that there was no risk of access to Netflix internal data, so we allowed the researcher to continue testing. After further testing, the researcher found a remote code execution that we then rewarded and remediated with a higher priority.</li><li>We always perform a full impact analysis to make sure we reward the researcher based on the actual impact vs demonstrated impact of any issue. In one example of this, a researcher found an endpoint that allowed access to an internal resource if the attacker knew a randomly generated identifier. The researcher had found the identifier via another endpoint, but had not explicitly linked the two findings. Given the potential impact, we asked the researcher to stop further testing. As we tested the first endpoint ourselves, we discovered this additional impact and issued a reward in accordance with the higher priority.</li></ul><p>Over the past year, we have received various high quality submissions from researchers, and we want to continue to engage with them to improve Netflix security. <a href="https://bugcrowd.com/todayisnew">Todayisnew</a> has been the highest earning researcher in our program over the last year. We recently revisited our overall reward ranges to make sure we are competitive with the market for our risk profile. In 2019, we also started publishing quarterly program updates to highlight new product areas for testing. Our goal is to keep the Netflix program an interesting and fresh target for bug bounty researchers.</p><p>Going into next year, our goal is to maintain the quality of the researcher experience in our program. We are also thinking about how to extend our bug bounty coverage to our studio app ecosystem. Last year, we conducted a bug bash specifically for some of our studio apps with researchers across the world. We found some significant issues through that and are exploring extending our program to some of our studio production apps in 2019. We thank all the researchers that have engaged in our program and look forward to continued collaboration with them to secure Netflix.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f075b49d1cb" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/netflix-public-bug-bounty-1-year-later-8f075b49d1cb">Netflix Public Bug Bounty, 1 year later</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Spinnaker Sets Sail to the Continuous Delivery Foundation]]></title>
            <link>https://medium.com/netflix-techblog/spinnaker-sets-sail-to-the-continuous-delivery-foundation-e81cd2cbbfeb?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/e81cd2cbbfeb</guid>
            <category><![CDATA[spinnaker]]></category>
            <category><![CDATA[continuous-delivery]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[open-feed]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 12 Mar 2019 16:16:00 GMT</pubDate>
            <atom:updated>2019-03-12T16:16:00.770Z</atom:updated>
            <content:encoded><![CDATA[<p><em>Author: </em><a href="https://twitter.com/aglover"><em>Andy Glover</em></a></p><p>Since releasing <a href="https://www.spinnaker.io/">Spinnaker</a> to the <a href="https://medium.com/netflix-techblog/global-continuous-delivery-with-spinnaker-2a6896c23ba7">open feed community in 2015</a>, the platform has flourished with the addition of new cloud providers, triggers, pipeline stages, and much more. Myriad new features, improvements, and innovations have been added by an ever growing, actively engaged community. Each new innovation has been a step towards an even better Continuous Delivery platform that facilitates rapid, reliable, <a href="https://medium.com/netflix-techblog/automated-canary-analysis-at-netflix-with-kayenta-3260bc7acc69">safe delivery</a> of flexible assets to pluggable deployment targets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*TXxqP2Q9pd_MsjCI2koL4Q.png" /></figure><p>Over the last year, Netflix has improved overall management of Spinnaker by enhancing community engagement and transparency. At the <a href="https://www.spinnakersummit.com/">Spinnaker Summit</a> in 2018, we announced that we had adopted a formalized <a href="https://www.spinnaker.io/community/governance/">project governance</a> plan with Google. Moreover, we also realized that we’ll need to share the responsibility of Spinnaker’s direction as well as yield a level of long-term strategic influence over the project so as to maintain a healthy, engaged community. This means enabling more parties outside of Netflix and Google to have a say in the direction and implementation of Spinnaker.</p><p>A strong, healthy, committed community benefits everyone; however, open feed projects rarely reach this critical mass. It’s clear Spinnaker has reached this special stage in its evolution; accordingly, we are thrilled to announce two exciting developments.</p><p>First, Netflix and Google are jointly donating Spinnaker to the newly created <a href="https://cd.foundation/">Continuous Delivery Foundation (or CDF)</a>, which is part of the Linux Foundation. The CDF is a neutral organization that will grow and sustain an open continuous delivery ecosystem, much like the Cloud Native Computing Foundation (or CNCF) has done for the cloud native computing ecosystem. The initial set of projects to be donated to the CDF are Jenkins, Jenkins X, Spinnaker, and Tekton. Second, Netflix is joining as a founding member of the CDF. Continuous Delivery powers innovation at Netflix and working with other leading practitioners to promote Continuous Delivery through specifications is an exciting opportunity to join forces and bring the benefits of rapid, reliable, and safe delivery to an even larger community.</p><p>Spinnaker’s success is in large part due to the amazing community of companies and people that use it and contribute to it. Donating Spinnaker to the CDF will strengthen this community. This move will encourage contributions and investments from additional companies who are undoubtedly waiting on the sidelines. Opening the doors to new companies increases the innovations we’ll see in Spinnaker, which benefits everyone.</p><p>Donating Spinnaker to the CDF doesn’t change Netflix’s commitment to Spinnaker, and what’s more, current users of Spinnaker are unaffected by this change. Spinnaker’s previously defined governance policy remains in place. Overtime, new stakeholders will emerge and play a larger, more formal role in shaping Spinnaker’s future. The prospects of an even healthier and more engaged community focused on Spinnaker and the manifold benefits of Continuous Delivery is tremendously exciting and we’re looking forward to seeing it continue to flourish.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e81cd2cbbfeb" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/spinnaker-sets-sail-to-the-continuous-delivery-foundation-e81cd2cbbfeb">Spinnaker Sets Sail to the Continuous Delivery Foundation</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[MezzFS — Mounting object storage in Netflix’s media processing platform]]></title>
            <link>https://medium.com/netflix-techblog/mezzfs-mounting-object-storage-in-netflixs-media-processing-platform-cda01c446ba?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/cda01c446ba</guid>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[video-encoding]]></category>
            <category><![CDATA[algorithms]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[amazon-s3]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 06 Mar 2019 16:10:17 GMT</pubDate>
            <atom:updated>2019-03-11T16:34:30.758Z</atom:updated>
            <content:encoded><![CDATA[<h3>MezzFS — Mounting object storage in Netflix’s media processing platform</h3><p><em>By </em><a href="https://www.linkedin.com/in/barakalon/"><em>Barak Alon</em></a><em> (on behalf of Netflix’s Media Cloud Engineering team)</em></p><p>MezzFS (short for “Mezzanine File System”) is a tool we’ve developed at Netflix that <a href="https://en.wikipedia.org/wiki/Mount_(computing)">mounts</a> <a href="https://en.wikipedia.org/wiki/Object_storage">cloud objects</a> as local files via <a href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace">FUSE</a>. It’s used extensively in our media processing platform, which includes services like <a href="https://medium.com/netflix-techblog/simplifying-media-innovation-at-netflix-with-archer-3f8cbb0e2bcb">Archer</a> and runs features like <a href="https://medium.com/netflix-techblog/high-quality-video-encoding-at-scale-d159db052746">video encoding</a> and <a href="https://medium.com/netflix-techblog/extracting-image-metadata-at-scale-c89c60a2b9d2">title image generation</a> on tens of thousands of <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> instances. There are similar tools out there, but we’ve developed some unique features like “replays” and “adaptive buffering” that we think are worth sharing.</p><h3>What problem are we solving?</h3><p>We are <a href="https://medium.com/netflix-techblog/tagged/video-encoding">constantly innovating</a> on video encoding technology at Netflix, and we have a lot of content to encode. Video encoding is what MezzFS was originally designed for and remains one of its canonical use cases, so we’ll focus on video encoding to describe the problem that MezzFS solves.</p><p>Video encoding is the process of converting an uncompressed video into a compressed format defined by a <a href="https://en.wikipedia.org/wiki/Video_codec">codec</a>, and it’s an essential part of preparing content to be streamed on Netflix. A single movie at Netflix might be encoded dozens of times for different codecs and video resolutions. Encoding is not a one-time process — large portions of the entire Netflix catalog are re-encoded whenever we’ve made significant advancements in encoding technology.</p><p>We scale out video encoding by processing segments of an uncompressed video (<a href="https://medium.com/netflix-techblog/optimized-shot-based-encodes-now-streaming-4b9464204830">we segment movies by scene</a>) in parallel. We have one file — the original, raw movie file — and many worker processes, all encoding different segments of the file. That file is stored in our object storage service, which splits and encrypts the file into separate chunks, storing the chunks in Amazon S3. This object storage service also handles content security, auditing, disaster recovery, and more.</p><p>The individual video encoders process their segments of the movie with tools like <a href="https://www.ffmpeg.org/">FFmpeg</a>, which doesn’t speak our object storage service’s API and expects to deal with a file on the local filesystem. Furthermore, the movie file is very large (often several 100s of GB), and we want to avoid downloading the entire file for each individual video encoder that might be processing only a small segment of the whole movie.</p><p>This is just one of many use cases that MezzFS supports, but all the use cases share a similar theme: stream the right bits of a remote object efficiently and expose those bits as a file on the filesystem.</p><h3>The solution: MezzFS</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/861/1*4WOBspuVg_3uiswXDNCItA.png" /></figure><p>MezzFS is a Python application that implements the FUSE interface. It’s built as a Debian package and installed by applications running on our media processing platform, which use MezzFS’s command line interface to mount remote objects as local files.</p><p>MezzFS has a number of features, including:</p><ul><li><strong>Stream objects</strong> —<strong> </strong>MezzFS exposes multi-terabyte objects without requiring any disk space.</li><li><strong>Assemble and decrypt parts </strong>— Our object storage service splits objects into many parts and stores them in S3. MezzFS knows how to assemble and decrypt the parts.</li><li><strong>Mount multiple objects </strong>—<strong> </strong>Multiple cloud objects can be mounted on the local filesystem simultaneously.</li><li><strong>Disk Caching </strong>—<strong> </strong>MezzFS can be configured to cache objects on the local disk.</li><li><strong>Mount ranges of objects </strong>—<strong> </strong>Arbitrary ranges of a cloud object can be mounted as separate files on the local file system. This is particularly useful in media computing, where it is common to mount the frames of a movie scene as separate files.</li><li><strong>Regional caching </strong>— Netflix operates in multiple AWS regions. If an application in region A is using MezzFS to read from an object stored in region B, MezzFS will cache the object in region A. In addition to improving download speed, this is useful for cutting down on cross-region transfer costs when many workers will be processing the same data — we only pay the transfer costs for one worker, and the rest use the cached object.</li><li><strong>Replays </strong>— More on this below…</li><li><strong>Adaptive buffering</strong> — More on this below…</li></ul><p>We’ve been using MezzFS in production for 5 years, and have validated it at scale — during a typical week at Netflix, MezzFS performs ~100 million mounts for dozens of different use cases and streams about ~25 petabytes of data.</p><h3>MezzFS “replays”</h3><p>MezzFS has become a crucial tool for us, and we don’t just send it out into the wild with a packed lunch and hope it will be fine.</p><p>MezzFS collects metrics on data throughput, download efficiency, resource usage, etc. in <a href="https://github.com/Netflix/atlas">Atlas</a>, Netflix’s in-memory dimensional time series database. Its logs are collected in an <a href="https://www.elastic.co/elk-stack">ELK</a> stack. But one of the more novel tools we’ve developed for debugging and developing is the MezzFS “replay”.</p><p>At mount time, MezzFS can be configured to record a “replay” file. This file includes:</p><ol><li><strong>Metadata</strong> — This includes: the remote objects that were mounted, the environment in which MezzFS is running, etc.</li><li><strong>File operations</strong> — All “open” and “read” operations. That is, all mounted files that were opened and every single byte range read that MezzFS received.</li><li><strong>Actions</strong> — MezzFS records everything it buffers and everything it caches</li><li><strong>Statistics</strong> — Finally, MezzFS will record various statistics about the mount, including: total bytes downloaded, total bytes read, total time spent reading, etc.</li></ol><p>A single replay may include million of file operations, so these files are packed in a custom binary format to minimize their footprint.</p><p>Based on these replay files, we’ve built tools that:</p><h4>Visualize a replay</h4><p>This has proven very useful for quickly gaining insight into data access patterns and why they might be causing performance issues.</p><p>Here’s a GIF of what these visualization look like:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*e7KHKRxBwN6qvADNAra7gw.gif" /><figcaption>Visualization of a MezzFS “replay”</figcaption></figure><p>The bytes of a remote object are represented by pixels on the screen, where the top left is the start of the remote object and the bottom right is the end. The different colors mean different things — green means the bytes have been scheduled for downloading, yellow means the bytes are being actively downloaded, blue means the bytes have been successfully returned, etc. What we see in the above visualization is a very simple access pattern — a remote object is mounted and then streamed through sequentially.</p><p>Here is a more interesting, “sparse” access pattern, and one that inspired “adaptive buffering” described later in this post. We can see lots of little green bars quickly sprinkle the screen — these bars represent the bytes that were downloaded:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ixJFhTbi7f7W8GhIzePpGA.gif" /><figcaption>Visualization of a sparse MezzFS “replay”</figcaption></figure><h4>Rerun a replay</h4><p>We mount the same objects and rerun all of the operations recorded in the replay file. We use this to debug errors and performance issues in specific mounts.</p><h4>Rerun a batch of replays</h4><p>We collect replays from actual MezzFS mounts in production, and we rerun large batches of replays for regression and performance tests. We’ve integrated these tests into our build pipeline, where a build will fail if there are any errors across the gamut of replays or if the performance of a new MezzFS commit falls below some threshold. We parallelize rerun jobs with <a href="https://netflix.github.io/titus/">Titus</a>, Netflix’s container management platform, which allows us to exercise many hundreds of replay files in minutes. The results are aggregated in <a href="https://www.elastic.co/">Elasticsearch</a>, allowing us to quickly analyze MezzFS’s performance across the entire batch.</p><h3>Adaptive Buffering</h3><p>These replays have proven essential for developing optimizations like “adaptive buffering”.</p><p>One of the challenges of efficiently streaming bits in a FUSE system is that the kernel will break reads into chunks. This means that if an application reads, for example, 1 GB from a mounted file, MezzFS might receive that as 16,384 consecutive reads of 64KB. Making 16,384 separate HTTP calls to S3 for 64KB will suffer significant overhead, so it’s better to “read ahead” larger chunks of data from S3, speeding up subsequent reads by anticipating that the data will be read sequentially. We call the size of the chunks being read ahead the “buffer size”.</p><p>While large buffer sizes speed up sequential data access, they can <em>slow down</em> “sparse” data access — that is, the application is not reading through the file consecutively, but is reading small segments dispersed throughout the file (as shown in the visualization above). In this scenario, most of the buffered data isn’t actually going to be used, leading to a lot of unnecessary downloading and very slow reads.</p><p>One option is to expect applications to specify a buffer size when mounting with MezzFS. This is not always easy for application developers to do, since applications might be using third party tools and developers might not actually know their access pattern. It gets even messier when an application changes access patterns during a single MezzFS mount.</p><p>With “adaptive buffering,” we aimed to make MezzFS “just work” for a variety of access patterns, without requiring application developers to maintain MezzFS configuration.</p><h4>How it works</h4><p>MezzFS records a sliding window of the most recent reads. When it receives a read for data that has not already been buffered, it calculates an appropriate buffer size. It does this by first grouping the window of reads into “clusters”, where a cluster is a contiguous set of reads.</p><p>Here’s an illustration of how reads relate to clusters:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*PIOClFeT4OPv5ydF8wwo8Q.png" /></figure><p>If the average number of bytes per read divided by the average number of bytes per cluster is close to 1, we classify the access pattern as “sparse”. In the “sparse” case, we try to match the buffer size to the average number of bytes per read. If number is closer to 0, we classify the access pattern as “dense”, and we set the buffer size to the maximum allowed buffer size divided by the number of clusters (We divide by the number of clusters to account for a common case when an application might have multiple threads all reading different parts from the same file, but each thread is reading its part “densely.” If we used the maximum allowed buffer size for each thread, our buffers would consume too much memory).</p><p>Here’s an attempt to represent this logic with some pseudo code:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4d344781b3d74b1a21a3cee6cafe7d7d/href">https://medium.com/media/4d344781b3d74b1a21a3cee6cafe7d7d/href</a></iframe><p>There is a limit on the throughput you can get out of a single HTTP connection to S3. So when the calculated buffer size is large, we divide the buffer into separate requests and parallelize them across multiple threads. So for “sparse” access patterns we improve performance by choosing a small buffer size, and for “dense” access patterns we improve performance by buffering lots of data in parallel.</p><h4>How much faster is this?</h4><p>We’ve been using adaptive buffering in production across a number of different use cases. For the purpose of clarity in demonstration, we used the “rerun a batch of replays” technique described above to run a quick and dirty test comparing the old buffering technique against the new.</p><p>Two replay files that represent two canonical access patterns were used:</p><ol><li><strong>Dense/Sequential</strong> — Sequentially read 1GB from a remote object.</li><li><strong>Sparse/Random</strong> — Read 32MB in chunks of 64KB, dispersed randomly throughout a remote object.</li></ol><p>And we compared two buffering strategies:</p><ol><li><strong>Fixed Sized Buffering</strong>— This is the old technique, where the buffer size is fixed at 8MB (we chose 8MB as a “one-size-fits-all” buffer size after running some experiments across MezzFS use cases at the time).</li><li><strong>Adaptive Buffering</strong>— The shiny new technique described above.</li></ol><p>We ran each combination of replay file and buffering strategy 10 times each inside containers with 2 Gbps network and 16 CPUs, recording the total time to process all the operations in the replay files. The following table represents the <em>minimum </em>of all 10 runs (while mean and standard deviation often seem like good aggregations, we use minimum here because variability is often caused by other processes interrupting MezzFS, or variability in network conditions outside of MezzFS’s control).</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d82b10b8de957be94f96fb4f76079226/href">https://medium.com/media/d82b10b8de957be94f96fb4f76079226/href</a></iframe><p>Looking at the dense/sequential replay, fixed buffering has a throughput of ~0.5 Gbps, while adaptive buffering has a throughput of ~1.1Gbps.</p><p>While a handful of seconds might not seem worth all the trouble, these seconds become hours for many of our use cases that stream significantly more bytes. And shaving off hours is especially beneficial in latency sensitive workflows, like encoding videos that are released on Netflix the day they are shot.</p><h3>Conclusion</h3><p>MezzFS has become a core part of our media transformation and innovation platform. We’ve built some pretty fancy tools around it that we’re actively using to quickly and confidently develop new features and optimizations.</p><p>The next big feature on our roadmap is support for writes, which has exciting potential for our next generation media processing platform and our growing, global network of movie production studios.</p><p><em>Netflix’s media processing platform is maintained by the Media Cloud Engineering (MCE) team. If you’re excited about large-scale distributed computing problems in media processing, </em><a href="https://jobs.netflix.com/search?q=%22media%20cloud%20engineering%22%20or%20%22content%20engineering%22"><em>we’re hiring</em></a><em>!</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cda01c446ba" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/mezzfs-mounting-object-storage-in-netflixs-media-processing-platform-cda01c446ba">MezzFS — Mounting object storage in Netflix’s media processing platform</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Design Principles for Mathematical Engineering in Experimentation Platform at Netflix]]></title>
            <link>https://medium.com/netflix-techblog/design-principles-for-mathematical-engineering-in-experimentation-platform-15b3ea143b1f?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/15b3ea143b1f</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[scientific-software]]></category>
            <category><![CDATA[causal-inference]]></category>
            <category><![CDATA[scientific-research]]></category>
            <category><![CDATA[experimentation]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Thu, 07 Mar 2019 20:26:06 GMT</pubDate>
            <atom:updated>2019-03-08T03:12:50.552Z</atom:updated>
            <content:encoded><![CDATA[<blockquote>Jeffrey Wong, Senior Modeling Architect, Experimentation Platform <br>Colin McFarland, Director, Experimentation Platform</blockquote><p>At Netflix, we have data scientists coming from many backgrounds such as neuroscience, statistics and biostatistics, economics, and physics; each of these backgrounds has a meaningful contribution to how experiments should be analyzed. To unlock these innovations we are making a strategic choice that our focus should be geared towards developing the surrounding infrastructure so that scientists’ work can be easily absorbed into the wider <a href="https://research.netflix.com/research-area/machine-learning-platform">Netflix Experimentation Platform</a>. There are 2 major challenges to succeed in our mission:</p><ol><li><em>We want to democratize the platform and create a contribution model: with a developer and production deployment experience that is designed for data scientists and friendly to the stacks they use.</em></li><li><em>We have to do it at Netflix’s scale: For hundreds of millions of users across hundreds of concurrent tests, spanning many deployment strategies from traditional A/B experiments, to evolving areas like </em><a href="https://medium.com/netflix-techblog/quasi-experimentation-at-netflix-566b57d2e362"><em>quasi experiments</em></a><em>.</em></li></ol><p>Mathematical engineers at Netflix in particular work on the scalability and engineering of models that estimate treatment effects. They develop scientific libraries that scientists can apply to analyze experiments, and also contribute to the engineering foundations to build a scientific platform where new research can graduate to. In order to produce software that improves a scientist’s productivity we have come up with the following design principles.</p><h3><strong>1. Composition</strong></h3><p>Data Science is a curiosity driven field, and should not be unnecessarily constrained<em>[</em><a href="https://multithreaded.stitchfix.com/blog/2019/01/18/fostering-innovation-in-data-science/"><em>1</em></a><em>]</em>. We support data scientists to have freedom to explore research in any new direction. To help, we provide software autonomy for data scientists by focusing on composition, a design principle popular in data science software like ggplot2 and dplyr<em>[</em><a href="https://speakerdeck.com/hadley/pipelines-for-data-analysis-in-r"><em>2</em></a><em>]</em>. Composition exposes a set of fundamental building blocks that can be assembled in various combinations to solve complex problems. For example, ggplot2 provides several lightweight functions like geom_bar, geom_point, geom_line, and theme, that allow the user to assemble custom visualizations; every graph whether simple or complex can be composed of small, lightweight ggplot2 primitives.</p><p>In the democratization of the experimentation platform we also want to allow custom analysis. Since converting every experiment analysis into its own function for the experimentation platform is not scalable, we are making the strategic bet to invest in building high quality causal inference primitives that can be composed into an arbitrarily complex analysis. The primitives include a grammar for describing the data generating process, generic counterfactual simulations, regression, bootstrapping, and more.</p><h3>2. Performance</h3><p>If our software is not performant it could limit adoption, subsequent innovation, and business impact. This will also make graduating new research into the experimentation platform difficult. Performance can be tackled from at least three angles:</p><h4>A) Efficient computation</h4><p>We should leverage the structure of the data and of the problem as much as possible to identify the optimal compute strategy. For example, if we want to fit ridge regression with various different regularization strengths we can do an SVD upfront and express the full solution path very efficiently in terms of the SVD.</p><h4>B) Efficient use of memory</h4><p>We should optimize for sparse linear algebra. When there are many linear algebra operations, we should understand them holistically so that we can optimize the order of operations and not materialize unnecessary intermediate matrices. When indexing into vectors and matrices, we should index contiguous blocks as much as possible to improve spatial locality<em>[</em><a href="https://people.eecs.berkeley.edu/~demmel/cs267_Spr99/Lectures/Lect_02_1999b.pdf"><em>3</em></a><em>]</em>.</p><h4>C) Compression</h4><p>Algorithms should be able to work on raw data as well as compressed data. For example, regression adjustment algorithms should be able to use frequency weights, analytic weights, and probability weights<em>[</em><a href="https://www.parisschoolofeconomics.eu/docs/dupraz-yannick/using-weights-in-stata(1).pdf"><em>4</em></a><em>]</em>. Compression algorithms can be lossless, or lossy with a tuning parameter to control the loss of information and impact on the standard error of the treatment effect.</p><h3>3. Graduation</h3><p>We need a process for graduating new research into the experimentation platform. The end to end data science cycle usually starts with a data scientist writing a script to do a new analysis. If the script is used several times it is rewritten into a function and moved into the Analysis Library. If performance is a concern, it can be refactored to build on top of high performance causal inference primitives made by mathematical engineers. This is the first phase of graduation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/978/0*4c41HzR4Bqn3mUnd" /></figure><p>The first phase will have a lot of iterations. The iterations go in both directions: data scientists can promote functions into the library, but they can also use functions from the library in their analysis scripts.</p><p>The second phase interfaces the Analysis Library with the rest of the experimentation ecosystem. This is the promotion of the library into the Statistics Backend, and negotiating engineering contracts for input into the Statistics Backend and output from the Statistics Backend. This can be done in an experimental notebook environment, where data scientists can demonstrate end to end what their new work will look like in the platform. This enables them to have conversations with stakeholders and other partners, and get feedback on how useful the new features are. Once the concepts have been proven in the experimental environment, the new research can graduate into the production experimentation platform. Now we can expose the innovation to a large audience of data scientists, engineers and product managers at Netflix.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vlVs0nqwJ2cMxYEo" /></figure><h3>4. Reproducibility</h3><p>Reproducibility builds trustworthiness, transparency, and understanding for the platform. Developers should be able to reproduce an experiment analysis report outside of the platform using only the backend libraries. The ability to replicate, as well as rerun the analysis programmatically with different parameters is crucial for agility.</p><h3>5. Introspection</h3><p>In order to get data scientists involved with the production ecosystem, whether for debugging or innovation, they need to be able to step through the functions the platform is calling. This level of interaction goes beyond reproducibility. Introspectable code allows data scientists to check data, the inputs into models, the outputs, and the treatment effect. It also allows them to see where the opportunities are to insert new code. To make this easy we need to understand the steps of the analysis, and expose functions to see intermediate steps. For example we could break down the analysis of an experiment as</p><ul><li><em>Compose data query</em></li><li><em>Retrieve data</em></li><li><em>Preprocess data</em></li><li><em>Fit treatment effect model</em></li><li><em>Use treatment effect model to estimate various treatment effects and variances</em></li><li><em>Post process treatment effects, for example with multiple hypothesis correction</em></li><li><em>Serialize analysis results to send back to the Experimentation Platform</em></li></ul><p>It is difficult for a data scientist to step through the online analysis code. Our path to introspectability is to power the analysis engine using python and R, a stack that is easy for a data scientist to step through. By making the analysis engine a python and R library we will also gain reproducibility.</p><h3>6. <strong>Scientific Code in Production and in Offline Environments</strong></h3><p>In the causal inference domain data scientists tend to write code in python and R. We intentionally are not rewriting scientific functions into a new language like Java, because that will render the library useless for data scientists since they cannot integrate optimized functions back into their work. Rewriting poses reproducibility challenges since the python/R stack would need to match the Java stack. Introspection is also more difficult because the production code requires a separate development environment.</p><p>We choose to develop high performance scientific primitives in C++, which can easily be wrapped into both python and R, and also delivers on highly performant, production quality scientific code. In order to support the diversity of the data science teams and offer first class support for hybrid stacks like python and R, we standardize data on the <a href="https://arrow.apache.org/">Apache Arrow</a> format in order to facilitate data exchange to different statistics languages with minimal overhead.</p><h3>7. Well Defined Point of Entry, Well Defined Point of Exit</h3><p>Our causal inference primitives are developed in a pure, scientific library, without business logic. For example, regression can be written to accept a feature matrix and a response vector, without any specific experimentation data structures. This makes the library portable, and allows data scientists to write extensions that can reuse the highly performant statistics functions for their own adhoc analysis. It is also portable enough for other teams to share.</p><p>Since these scientific libraries are decoupled from business logic, they will always be sandwiched in any engineering platform; upstream will have a data layer, and downstream will have a visualization and interpretation layer. To facilitate a smooth data flow, we need to design simple connectors. For example, all analyses need to receive data and a description of the data generating process. By focusing on composition, an arbitrary analysis can be constructed by layering causal analysis primitives on top of that starting point. Similarly, the end of an analysis will always consolidate into one data structure. This simplifies the workflow for downstream consumers so that they know what data type to consume.</p><h3>Next Steps</h3><p>We are actively developing high performance software for regression, heterogeneous treatment effects, longitudinal studies and much more for the Experimentation Platform at Netflix. We aim to accelerate research in causal inference methodology, expedite product innovation, and ultimately bring the best experience and delight to our members. This is an ongoing journey, and if you are passionate about our exciting work, <a href="https://research.netflix.com/research-area/machine-learning-platform">join our all-star team</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=15b3ea143b1f" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/design-principles-for-mathematical-engineering-in-experimentation-platform-15b3ea143b1f">Design Principles for Mathematical Engineering in Experimentation Platform at Netflix</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Trace Event, Chrome and More Profile Formats on FlameScope]]></title>
            <link>https://medium.com/netflix-techblog/trace-event-chrome-and-more-profile-formats-on-flamescope-5dfe9df5dfa9?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/5dfe9df5dfa9</guid>
            <category><![CDATA[performance]]></category>
            <category><![CDATA[chrome]]></category>
            <category><![CDATA[linux]]></category>
            <category><![CDATA[javascript]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 06 Mar 2019 18:01:50 GMT</pubDate>
            <atom:updated>2019-03-06T18:01:50.493Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9kWnsmLKRcz3_ZEp" /><figcaption>FlameScope sub-second heatmap visualization.</figcaption></figure><p>Less than a year ago, <a href="https://medium.com/netflix-techblog/netflix-flamescope-a57ca19d47bb">FlameScope</a> was released as a proof of concept for a new profile visualization. Since then, it helped us, and many other users, to easily find and fix performance issues, and allowed us to see patterns that we had never noticed before in our profiles.</p><p>As a tool, FlameScope was limited. It only supported the profile format generated by Linux perf, which at the time, was the profiler of choice internally at Netflix.</p><p>Immediately after launch, we received multiple requests to support other profile formats. Users looking to use the FlameScope visualization, with their own profilers and tools. Our goal was never to support hundreds of profile formats, especially for tools we don’t use internally, but we always knew that supporting a few “generic” formats would be useful, both for us, and the community.</p><p>After receiving multiple requests from users and investigating a few profile formats, we opted to support the <a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview">Trace Event Format</a>. It is well documented. It is flexible. Multiple tools already use it, and it is the format used by <a href="https://github.com/catapult-project/catapult/tree/master/tracing">Trace-Viewer</a>, which is the javascript frontend for Chrome’s <a href="http://dev.chromium.org/developers/how-tos/trace-event-profiling-tool">about:tracing</a> and <a href="http://developer.android.com/tools/help/systrace.html">Android’s systrace</a> tools.</p><p>The complete documentation for the format can be found <a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview">here</a>, but in a nutshell, it consists of an ordered list of events. For now, FlameScope only supports <em>Duration</em> and <em>Complete</em> event types. According to the documentation:</p><blockquote>Duration events provide a way to mark a duration of work on a given thread. The duration events are specified by the B and E phase types. The B event must come before the corresponding E event. You can nest the B and E events. This allows you to capture function calling behavior on a thread.</blockquote><blockquote>Each complete event logically combines a pair of duration (B and E) events. The complete events are designated by the X phase type.</blockquote><blockquote>There is an extra parameter dur to specify the tracing clock duration of complete events in microseconds. All other parameters are the same as in duration events.</blockquote><p>Here’s an example:</p><pre>[<br>  {</pre><pre>    &quot;name&quot;: &quot;Asub&quot;,</pre><pre>    &quot;cat&quot;: &quot;PERF&quot;,</pre><pre>    &quot;ph&quot;: &quot;B&quot;,</pre><pre>    &quot;pid&quot;: 22630,</pre><pre>    &quot;tid&quot;: 22630,</pre><pre>    &quot;ts&quot;: 829</pre><pre>  }, {</pre><pre>    &quot;name&quot;: &quot;Asub&quot;,</pre><pre>    &quot;cat&quot;: &quot;PERF&quot;,</pre><pre>    &quot;ph&quot;: &quot;E&quot;,</pre><pre>    &quot;pid&quot;: 22630,</pre><pre>    &quot;tid&quot;: 22630,</pre><pre>    &quot;ts&quot;: 833</pre><pre>  }<br>]</pre><p>As you can imagine, this format works really well for tracing profilers, where the beginning and end of work units are recorded. For sampling based profilers, like perf, the format is not ideal. We could create a Complete event for every sample, with stacks, but even being more efficient than the output generated by perf, there is still a lot of overhead, especially from repeated stacks. Another option would be to analyze the whole profile and create begin and end events every time we enter or exit a stack frame, but that adds complexity to converters.</p><p>Since we also work with sampling profilers frequently, we needed a simpler format. In the past, we worked with profiles in the v8 profiler format, which is very similar to Chrome’s old JavaScript CPU profiler format and newer <a href="https://chromedevtools.github.io/devtools-protocol/tot/Profiler#type-Profile">ProfileType</a> event format. We already had all the code needed to generate both heatmap and partial flame graphs, so we decided to use it as base for a new format, which for lack of a more creative name, we called <em>nflxprofile.</em> Different from the v8 profiler format, it uses a map instead of a list to store the nodes, includes extra information about the profile, and takes advantage <a href="https://developers.google.com/protocol-buffers/">Protocol Buffers</a> to serialize the data instead of JSON. The <em>.proto</em> file looks like this:</p><pre><em>syntax = “proto2”;</em></pre><pre><em>package nflxprofile;</em></pre><pre><em>message Profile {</em></pre><pre><em>  required double start_time = 1;</em></pre><pre><em>  required double end_time = 2;</em></pre><pre><em>  repeated uint32 samples = 3 [packed=true];</em></pre><pre><em>  repeated double time_deltas = 4 [packed=true];</em></pre><pre><em>  message Node {</em></pre><pre><em>    required string function_name = 1;</em></pre><pre><em>    required uint32 hit_count = 2;</em></pre><pre><em>    repeated uint32 children = 3;</em></pre><pre><em>    optional string libtype = 4;</em></pre><pre><em>  }</em></pre><pre><em>  map&lt;uint32, Node&gt; nodes = 5;</em></pre><pre><em>}</em></pre><p>It can be found on <a href="https://github.com/Netflix/flamescope">FlameScope’s repository</a> too, and be used to generate code for the multiple programming languages supported by Protocol Buffers.</p><p>Netflix has been using the new format internally in its cloud profiling tool, and the improvement is noticeable. The significant reduction in file size, from raw perf output to <em>nflxprofile</em>, allows for faster download time from external storage. The reduction will depend on sampling duration and how homogeneous the workload is (similar stacks), but generally, the output is orders of magnitude smaller. Time spent on parsing and deserialization is also reduced significantly. No more regular expressions!</p><p>Since the new format is so similar to what Chrome generates, we decided to include it too! It has been challenging to keep up with the constant changes in DevTools, from <em>CpuProfile</em>, to <em>Profile</em> and now <em>ProfileChunk</em> events, but the format is supported as of now. If you want to try it out, check out the <a href="https://developers.google.com/web/tools/chrome-devtools/evaluate-performance/">Get Started With Analyzing Runtime Performance</a> post, record and save the profile to FlameScope’s profile directory, and open it!</p><p>We also had to make minor adjustments to the user interface, more specifically the file list, to support the new profile formats. Now, instead of a simple list, you will get a dropdown menu next to each file that allows you to select the correct profile type.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/251/0*BM6VQY-_vjlhA7YT" /><figcaption>New profile selection dropdown.</figcaption></figure><p>We might consider adding support for more formats in the future, or accept pull requests that add support for something new, but in general, profile converters are the simplest solution. If you created a converter for a known profile format, we are happy to link to it on FlameScope’s documentation!</p><p><em>FlameScope was developed by Martin Spier, Brendan Gregg and the Netflix performance engineering team. Blog post by Martin Spier.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5dfe9df5dfa9" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/trace-event-chrome-and-more-profile-formats-on-flamescope-5dfe9df5dfa9">Trace Event, Chrome and More Profile Formats on FlameScope</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Data Inspires Building a Scalable, Resilient and Secure Cloud Infrastructure At Netflix]]></title>
            <description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://medium.com/netflix-techblog/how-data-inspires-building-a-scalable-resilient-and-secure-cloud-infrastructure-at-netflix-c14ea9f2d00c?feed=rss----2615bd06b42e---4"><img src="https://cdn-images-1.medium.com/max/1280/1*Tm3SDN2KKT0v29aTGZrnmA.png" width="1280"></a></p><p class="medium-feed-snippet">Netflix&#x2019;s engineering culture is predicated on Freedom &amp; Responsibility, the idea that everyone (and every team) at Netflix is entrusted&#x2026;</p><p class="medium-feed-link"><a href="https://medium.com/netflix-techblog/how-data-inspires-building-a-scalable-resilient-and-secure-cloud-infrastructure-at-netflix-c14ea9f2d00c?feed=rss----2615bd06b42e---4">Continue reading on Netflix TechBlog »</a></p></div>]]></description>
            <link>https://medium.com/netflix-techblog/how-data-inspires-building-a-scalable-resilient-and-secure-cloud-infrastructure-at-netflix-c14ea9f2d00c?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/c14ea9f2d00c</guid>
            <category><![CDATA[reliability-engineering]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[security]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 05 Mar 2019 16:06:00 GMT</pubDate>
            <atom:updated>2019-03-23T06:01:12.079Z</atom:updated>
        </item>
        <item>
            <title><![CDATA[Extending Vector with eBPF to inspect host and container performance]]></title>
            <link>https://medium.com/netflix-techblog/extending-vector-with-ebpf-to-inspect-host-and-container-performance-5da3af4c584b?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/5da3af4c584b</guid>
            <category><![CDATA[containers]]></category>
            <category><![CDATA[bpf]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[linux]]></category>
            <category><![CDATA[performance]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 20 Feb 2019 17:51:57 GMT</pubDate>
            <atom:updated>2019-02-20T17:51:57.469Z</atom:updated>
            <content:encoded><![CDATA[<p><em>by </em><a href="https://www.linkedin.com/in/jason-koch-5692172/"><em>Jason Koch</em></a><em>, with </em><a href="https://www.linkedin.com/in/martinspier"><em>Martin Spier</em></a><em>, </em><a href="http://www.brendangregg.com/"><em>Brendan Gregg</em></a><em>, </em><a href="https://www.linkedin.com/in/edwhunter/"><em>Ed Hunter</em></a></p><p>Improving the tools available to our engineers to help them diagnose, triage, and work through software performance challenges in the cloud is a key goal for the cloud performance engineering team at Netflix.</p><p>Today we are excited to announce latency heatmaps and improved container support for our on-host monitoring solution — Vector — to the broader community. Vector is open feed and in use by multiple companies. These updates also bring other user experience improvements and a fresher technology stack.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SOK2b3XUeMRlZUYUInLMxQ.gif" /><figcaption>Remotely view real-time process scheduler latency and tcp throughput with Vector and eBPF</figcaption></figure><p><strong>What is Vector?</strong></p><p>Vector is an open-feed host-level performance monitoring framework which we have been using for some time. Having the right metrics available on demand and at a high resolution is key to understanding how a system behaves and helps to quickly troubleshoot performance issues. For more information on the background and architecture of Vector and PCP, you can see our earlier tech blog post “<a href="https://medium.com/netflix-techblog/introducing-vector-netflixs-on-host-performance-monitoring-tool-c0d3058c3f6f">Introducing Vector</a>”.</p><p><strong>Why the changes</strong></p><p>There are two main triggers for the refresh:</p><ul><li>Firstly, at Netflix we are seeing significant growth in the use of our container environment (Titus). Historically the focus of our tooling and platform has been on AWS EC2, but with more users making the migration, we need to provide better support for container use cases.</li><li>For example, being able to present both host and container level metrics on the same dashboard helps us provide a way to quickly answer questions: “am I being throttled by the container runtime?” or “are there noisy neighbors affecting my container task?”.</li><li>Secondly, we have a collection of open requests and issues that were difficult to resolve with the current dashboard component. There was no way to pause graphs to take screenshots, and chart legends were sometimes laid out obscuring the actual chart data.</li></ul><p><strong>Introducing BCC / eBPF visualisations</strong></p><p>The Netflix performance engineering team (especially my colleague Brendan) has been contributing to eBPF since its beginning in 2014, including developing many open feed performance tools for BCC such as execsnoop, biosnoop, biotop, ext4slower, tcplife, runqlat, and more. These are command line tools, and for them to be really practical in the Netflix cloud environment of over 100k instances, they need to be runnable from our self service GUIs including Vector. For that to work, there needed to be a PCP interface for BPF.</p><p><a href="https://github.com/andihit">Andreas Gerstmayr</a> has done fantastic work with developing a PCP PMDA for BCC, allowing BCC tool output to be read from Vector, and adding visualizations to Vector to view this data. Vector can now show these from BCC/eBPF:</p><ul><li>Block and filesystem (ext4, xfs, zfs) latency heat maps</li><li>Block IO top processes</li><li>Active TCP session data such as top users, session life, retransmits ..</li><li>Snoops: block IO, exec()</li><li>Scheduler run queue latency</li></ul><p>In the below diagram we can see a demo for a wget job. As soon as the wget job starts, the ‘runqlat’ chart shows increased scheduler activity (more yellow areas in the diagram) and longer queue latency for some processes (blue blocks appear higher in the vertical area of the chart). The ‘tcptop’ also shows a new process appearing, (wget) with a new TCP connection and a significant data in RX_KB — 10–20 MB/sec (it is, unsurprisingly, receiving lots of data).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SOK2b3XUeMRlZUYUInLMxQ.gif" /><figcaption>Real-time scheduler run queue latency and tcp throughput charts after starting a wget download</figcaption></figure><p>Many thanks to the great work by Andreas during his 2018 Google Summer of Code project.</p><p><strong>New features</strong></p><p>To address these changes, we have introduced the following new features:</p><ul><li>We are now able to visualize any combination of host and container metrics from a single view. This allows us to create more complex single-pane dashboards. For example, charting host CPU alongside container CPU, or charting network IO for two communicating instances on the same visualization can be an easy way to get better insights.</li><li>Charts are now resizable and movable.<br>This makes it much easier for engineers to get the graphs they want arranged in a manner for comparisons and to focus in the required areas. In this example, CPU utilisation from two different hosts are arranged so that correlation of spikes — or not — is more clearly visible.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8R5QV6wYmg4jKZ-P3kiRZA.gif" /><figcaption>Charts are now resizable and movable</figcaption></figure><ul><li>Charts will keep collecting data even when the tab is not visible.<br>Previously, switching to a different browser tab paused graph data collection and generation. Now, when the tab is not visible, data will still be collected even though rendering is paused.</li></ul><p><strong>User experience improvements</strong></p><ul><li>Legends have been moved to tool tips.<br>This frees up some of visual real estate and removes render issues with large legends consuming the chart area, as you can see here:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*OBNqoU1_dji_oSqk5_IYXw.gif" /><figcaption>Tooltips now show point in time metrics</figcaption></figure><ul><li>Graphs can be paused and resumed.<br>This is helpful for taking screenshots, when comparing data points across charts, or in discussion with other engineers. When Pause is clicked, data collection continues in the background and graph updates are held. Graphs are brought immediately up to date with live data when Play is clicked again.<br>Here, you can see the Pause button pressed, and the graph pauses until the user hits the Play button.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/998/1*FKKpNVhgteSbE3uHEwKpSQ.gif" /><figcaption>A pause button has been introduced to help compare, take screenshots, etc.</figcaption></figure><ul><li>Styling and widgets<br>Introduction of new styling and configuration approach. We have introduced the <a href="https://arxiv.org/ftp/arxiv/papers/1712/1712.01662.pdf">Cividis color scheme</a> for heat maps.</li></ul><p><strong>Internal technology refresh</strong></p><p>The earlier dashboard core components were running on a tech stack that was — for JavaScript — a relatively old Angular 1.x deployment, with a number of component dependencies. A key dashboard component is no longer maintained; new features we would need to resolve issues were not available without forking the component. Instead of forking and investing in a deprecated stack, we have decided on a stack refresh:</p><ul><li>Switch from Angular 1.x to React w/ Semantic UI.</li><li>Refresh the build pipeline away from gulp to webpack.</li><li>Introduction of <a href="https://github.com/emeeks/semiotic/">Semiotic</a> which is a powerful graph render layer over React and pieces of d3. Along the way we made a bundle of performance improvements to Semiotic for our use case too.</li></ul><p><strong>Current state</strong></p><p>This code is now available for public consumption and should be making its way to distro packages in their release cycle over time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/485/1*-TmOTMPRPW6aSi9bb1-rTg.png" /></figure><p>As always, your feedback and contributions are very much appreciated. The best way is to reach out to us on Github: <a href="http://github.com/netflix/vector">http://github.com/netflix/vector</a>, or on Slack: <a href="https://vectoross.slack.com/">https://vectoross.slack.com/</a>.</p><p><strong>Looking forward</strong></p><p>Vector continues to serve internal Netflix users. There are a number of user-experience improvements that can be made to the new tooling. In addition to this, the continued shift to containers, with a focus on container startup time and short-lived containers are likely to drive tighter integration with container scheduling tools. We are also excited to see the future of monitoring with more heterogeneous workloads — not only Intel x86, but also ARM, AMD x86, GPUs, FPGAs, etc.</p><p>If you’re interested in working on performance challenges and the idea of building quality tools, visualizations appeals to you — please get in touch with us, we’re hiring!</p><ul><li><a href="https://jobs.netflix.com">https://jobs.netflix.com</a> — Netflix</li><li><a href="https://jobs.netflix.com/jobs/865018">https://jobs.netflix.com/jobs/865018</a> — Senior Performance Engineer</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5da3af4c584b" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/extending-vector-with-ebpf-to-inspect-host-and-container-performance-5da3af4c584b">Extending Vector with eBPF to inspect host and container performance</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building a Cross-platform In-app Messaging Orchestration Service]]></title>
            <link>https://medium.com/netflix-techblog/building-a-cross-platform-in-app-messaging-orchestration-service-86ba614f92d8?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/86ba614f92d8</guid>
            <category><![CDATA[in-app-messaging]]></category>
            <category><![CDATA[messaging]]></category>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[product-design]]></category>
            <category><![CDATA[product-development]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Mon, 11 Feb 2019 17:01:00 GMT</pubDate>
            <atom:updated>2019-02-11T17:01:00.934Z</atom:updated>
            <content:encoded><![CDATA[<h4><a href="https://www.linkedin.com/in/imgeorgeabraham/">George Abraham</a>, <a href="https://www.linkedin.com/in/devika-chawla-8418b33/">Devika Chawla</a>, <a href="https://www.linkedin.com/in/chrisnbeaumont/">Chris Beaumont</a>, and <a href="https://www.linkedin.com/in/yuewangh/">Daniel Huang</a>.</h4><p>Thoughtful, relevant, and timely messaging is an integral part of a customer’s Netflix experience. The Netflix Messaging Engineering team builds the platform and the messages to communicate with Netflix customers.</p><h3>Messages in the Netflix App</h3><p>In-app messages at Netflix fall broadly into two channels — Notifications and Alerts. Notifications are content recommendations that show up in the Notification Center within the Netflix app. Alerts are typically account related messages that are designed to draw the customer’s attention to take some action and show up on the main screen. This blog post will focus on the Alerts channel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*UoJijavpw3SYpXjb2fvDRg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*U8chrRjS6vXvQhbNWysrUQ.jpeg" /><figcaption>Left: Notification Center on the Netflix iOS app. Right: An in-app Alert on the Netflix iOS app.</figcaption></figure><h3>Some History</h3><p>It is worth spending some time painting a picture of the landscape that we evolved from. In the early days, account-related Alerts were served only on the Netflix website app. All the logic around presenting an Alert was implemented in the website codebase. The UI handled business logic around priority, customer state validation, timing, etc. A few years ago, the Netflix website was undergoing a major re-architecture. We simplified the ecosystem and built an in-app messaging service that handled the complexity of messaging orchestration by taking ownership of dimensions like</p><ul><li>Timing — when to show a message</li><li>Frequency — how often to show a message</li><li>Device Eligibility — which devices to show the message</li><li>Population Segment — which profiles to show the message</li></ul><p>This freed up UI platforms to focus their attention on core UI innovation.</p><h3>Initial Goals</h3><p>Our primary goals at project inception were the following:</p><ol><li>Continue to support in-app Alert messaging in the new Website architecture (i.e remain backward compatible)</li><li>Transfer business logic around messaging orchestration from the UI tier to the in-app messaging service</li><li>Support cross-platform messaging</li><li>Minimize time spent on running and productizing messaging A/B tests.</li></ol><h3>Messaging Architecture Overview</h3><p>Messaging infrastructures services are implemented in Java and like most Netflix services are deployed to AWS on clusters of EC2 instances in multiple AWS regions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*35SNXLrIDOz14Fw0kWpB0g.png" /><figcaption>A simplified view of the messaging platform</figcaption></figure><p>Events are consumed by the messaging platform where they are transformed into fully formed messages that are routed to either the external message delivery service or the in-app message delivery service.</p><p>For external channels, a message is created by calling other Netflix services for relevant information, assembled in the appropriate format and delivered right away to the external service (e.g. Apple Push Notification Service for iOS push notifications).</p><p>For the in-app channels, a message skeleton is stored in the service where it resides until a customer logs into the Netflix app — at which point the message is validated, assembled, and delivered to the device to be displayed.</p><h3>In-app Messaging Service</h3><p>At Netflix, member UI teams are organized by the platform that they work on (Android, iOS, TV, Website, etc). Each platform technology stack is different and the treatment of an Alert on each platform looks significantly different. Creating custom payload contracts for each platform would be an inefficient and error-prone solution. It would also hinder our velocity to test messaging experiences across platforms.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XbRedsUzkmlTaScVlIpibw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*85U5GfHuBfrKxr6XPbn3hA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EyTrORrwnl_-a4nmRFpHSQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/896/1*SJ3smlL0PIGd8eIfV9RDMQ.png" /><figcaption>A mock Netflix Alert on various Netflix devices — clockwise from top: TV, iOS, Web, Android</figcaption></figure><h4>Design Based Payload Contract</h4><p>We settled on a custom JSON payload contract that has pieces that are common to all the UIs but also accounted for differences in the design. We structured it in a way that UI platforms can implement the rendering of an in-app Alert without having to know anything about the specific message type. From the UI standpoint — there is <strong>an</strong> Alert to be shown.</p><pre>{<br>   &quot;templateId&quot;: &quot;standard&quot;, <br>   &quot;<a href="https://sites.google.com/netflix.com/umsalerts/payload/template">template</a>&quot;: { }, <br>   &quot;<a href="https://sites.google.com/netflix.com/umsalerts/payload/attributes">attributes</a>&quot;: { }<br>}</pre><p><strong>The Template Identifier</strong></p><p>The <em>templateId</em> is an identifier that lets the UI platform decide how to render the Alert. In simple terms — the design that should be used to render this Alert. In this example, the payload indicates to use the <em>standard</em> design. Design elements such as color, font, etc. are not prescribed since they are baked into the rendering logic that UI platforms use when they recognize the <em>standard</em> templateId.</p><p><strong>The Template</strong></p><p>The <em>template</em> field contains the payload that describes the various elements of the Alert for the specific <em>templateId</em>. Each field within the <em>template</em> corresponds to an element of the Alert with appropriate copy and attributes.</p><pre>{<br>   &quot;template&quot;: {<br>      &quot;title&quot;: { <br>         &quot;<a href="https://sites.google.com/netflix.com/umsalerts/payload/copy-markup">copy</a>&quot;: [] <br>      },<br>      &quot;body&quot;: {<br>         &quot;<a href="https://sites.google.com/netflix.com/umsalerts/payload/copy-markup">copy</a>&quot;: []<br>      },<br>      &quot;ctas&quot;: [<br>         {},{},{}<br>      ]<br>   },<br>   &quot;footer&quot;: {<br>      &quot;<a href="https://sites.google.com/netflix.com/umsalerts/payload/copy-markup">copy</a>&quot;: [ ]<br>   }<br>}</pre><p><strong>Copy</strong></p><p>The messaging service also took responsibility for delivering the localized strings for the Alert to the device. This allowed us to create more complex messaging experiences for customers that were not possible before. For instance, the in-app messaging service is now able to intelligently adjust the copy, cadence, etc. associated with a message based on user interaction across platforms. Because the messaging service hosts the strings on the service — we are also not tied to release cycles of each UI platform.</p><p>The payload also allows for basic markups such as boldface, newline, and links, since these are typically used within the copy itself. This approach provides reasonable customization of commonly used copy elements across platforms but keeps the messaging system out of the path of design innovations in the UI. Most importantly, this approach also accounts for differences in the technology stack in each UI platform.</p><pre>&quot;copy&quot; : [ {<br>      &quot;elementType&quot; : &quot;TEXT&quot;,<br>      &quot;content&quot; : &quot;This is an example &quot; <br>  }, {<br>      &quot;elementType&quot; : &quot;BOLD&quot;,<br>      &quot;content&quot; : &quot;Netflix Alert Message&quot;<br>   }, {<br>      &quot;elementType&quot; : &quot;TEXT&quot;,<br>      &quot;content&quot; : &quot;.&quot;<br>   }]<br>}</pre><p><strong>Calls to Action</strong></p><p>Calls to action (CTAs) include both copy and action attributes</p><pre>{<br>   &quot;actionType&quot;: &quot;BACKGROUND_SERVICE_CALL&quot;,<br>   &quot;action&quot;: &quot;DISMISS_ALERT&quot;,<br>   &quot;ctaType&quot; : &quot;BUTTON&quot;,<br>   &quot;copy&quot;: [], <br>   &quot;isSelected&quot;: true,<br>   &quot;ctaFeedback&quot;: { } <br>}</pre><p>An <em>actionType</em> is a top-level category to denote what type of action should be taken when a customer clicks the CTA. Some clicks result in a redirect to another flow in the app while others may result in a call to backend service to update something.</p><p>The <em>action</em> describes the specific action to be taken when a customer clicks on the CTA. For instance, one <em>action</em> could redirect the user to the <strong>plan selection</strong> flow whereas another could redirect the customer to the <strong>change email</strong> flow.</p><p>For designs that support multiple CTAs — the payload dictates which CTA to highlight by default with the <em>isSelected</em> flag.</p><p>Each CTA also contains a feedback field that is posted back to the service in its entirety. The contents of the feedback are not inspected by the UI platform, it is simply a passthrough that is used by the in-app messaging service to implement the orchestration features of the service.</p><pre>{<br>   &quot;ttl&quot;: 3600,<br>   &quot;feedbackType&quot;: &quot;cta&quot;,<br>   &quot;cta&quot;: &quot;DISMISS_ALERT&quot;, <br>   &quot;trackingInfo&quot;: {<br>      &quot;messageGuid&quot;: &quot;786DECAE429EEB029EEE057191675F6764555F12&quot;,<br>      &quot;eventGuid&quot;: &quot;D6AC068F169C0E616E99AF1C72F1AD8264555F12&quot;,<br>      &quot;renderGuid&quot;: &quot;85661FCF-0857-42A7-AA0E-559A26A5723B_R&quot;,<br>      &quot;messageName&quot;: &quot;EXAMPLE_ALERT&quot;,<br>      &quot;messageId&quot;: 1234,<br>      &quot;locale&quot;: &quot;en-GB&quot;,<br>      &quot;abTestId&quot;: 2222,<br>      &quot;abTestCell&quot;: 2,<br>      &quot;templateId&quot;: &quot;standard&quot;,<br>   }<br>}</pre><p>In the feedback call to the service, the UI platform also passes additional context that the messaging service is not privy to — like UI platform, device id, application version, etc.</p><p>The data here allows the service to perform orchestration features like changing the behavior of subsequent impressions of the Alert based on the customer’s prior interactions. For instance — we can suppress the Alert for a day on all devices if a customer clicked on DISMISS on any device. Or, we can choose to show a follow-up Alert using a less interruptive design with different copy limited to certain devices.</p><p><strong>Attributes</strong></p><p>The <em>attributes</em> field contains information to help UI platforms drive the rendering and user experience of the Alert. These are typically platform-specific attributes — such as blacklisted pages for an Alert on the website. For instance, we don’t want to show an “Add Your Phone Number” Alert on the Add Phone page.</p><p>The attributes section also contains a feedback field that is posted back to the service once the Alert is rendered. This feedback allows the in-app messaging service to change the behavior based on just the impression of the Alert even if the customer did not click a CTA.</p><h3>Challenges</h3><p>One of the main challenges we faced when starting down this path was that messaging engineers had little to no knowledge about UI platforms and the nuances among them. We had to learn to account for differences such as message fetching patterns, testing infrastructure, bandwidth, refresh times, etc. We also evolved our payload from a one size fits all approach to a more balanced one by taking on some of the complexity and managing the nuances by implementing a UI-centric design based payload.</p><p>Another challenge was around test automation. Our early days were filled with running manual tests because we didn’t have the infrastructure in place to integrate with UI automation frameworks. In the past year, our investment in test automation has resulted in shortening the time taken for validating changes.</p><p>Finally, localization QC was a challenge because we didn’t have the tooling in place to take screenshots across the various UIs for a message. We now have a workflow that allows the localization team to generate Alert screenshots for all languages and devices.</p><h3>Wins</h3><p>The in-app messaging service has opened up avenues to run more A/B tests than was possible before.</p><ul><li>We now build cross-platform experiences so that customers can get a consistent messaging experience on all devices.</li><li>We have reduced the development time for messaging experimentation because resources and context for these projects are contained within the messaging team.</li><li>We run omnichannel messaging tests that incorporate other messaging channels such as email, push notifications, etc.</li><li>We run experiments using levers such as timing, channel selection, frequency, etc. since the in-app messaging service is integrated into the Netflix customer messaging platform.</li></ul><h3>Looking Ahead</h3><p>As Netflix grows around the world, it’s increasingly beneficial and convenient to communicate with our customers inside the Netflix app. From account related messages to onboarding, and more — we are continually evolving our message portfolio.</p><p>One area where we are looking to reduce overall development time is around how UI platforms handle interaction with CTAs. Currently, for CTAs that result in a background service call, the UI platform maintains the code that is hosted in the<a href="https://medium.com/netflix-techblog/engineering-trade-offs-and-the-netflix-api-re-architecture-64f122b277dd"> Netflix API Platform</a>. It was the path of least resistance when we started since most platforms were already integrated with backend systems. As our systems grow and become more complex, making changes to these flows will become harder.</p><p>We are evolving the service to a completely federated architecture for in-app messaging orchestration where UI platforms will need to interface only with the in-app messaging service eliminating the need for them to create and maintain integrations with backend services for each kind of CTA. At that point, as far as the UI platform is concerned — the payload contract for the Alert will specify what information needs to be passed back to the messaging service which will handle all the downstream calls, provide fallbacks, etc.</p><p><strong>If you would like to join our small, impactful, and collaborative team — come </strong><a href="https://jobs.netflix.com/search?q=Messaging&amp;organization=Engineering"><strong>join us</strong></a><strong>.</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=86ba614f92d8" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/building-a-cross-platform-in-app-messaging-orchestration-service-86ba614f92d8">Building a Cross-platform In-app Messaging Orchestration Service</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Protecting a Story’s Future with History and Science]]></title>
            <link>https://medium.com/netflix-techblog/protecting-a-storys-future-with-history-and-science-e21a9fb54988?feed=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/e21a9fb54988</guid>
            <category><![CDATA[color-science]]></category>
            <category><![CDATA[color-management]]></category>
            <category><![CDATA[post-production]]></category>
            <category><![CDATA[film-history]]></category>
            <category><![CDATA[film-and-television]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 05 Feb 2019 18:28:28 GMT</pubDate>
            <atom:updated>2019-02-21T17:16:33.012Z</atom:updated>
            <content:encoded><![CDATA[<p><em>By Kylee Peña, Chris Clark, and Mike Whipple</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/375/1*mxtv8A_-p6vjqHMybq8GjA.png" /><figcaption>Kylee’s parents after their wedding in 1978.</figcaption></figure><p>I — Kylee — have two photos from my parents’ wedding. Just two. This year they celebrated 40 years of marriage, so both photos were shot on film. Both capture a joy and awkwardness that come with young weddings. They’re fresh and full of life, candid captures from another era.</p><p>One of the photos is a Polaroid Instant Camera shot. It’s the only copy of the photo. The colors are beginning to fade, and the image itself is only three or four inches across.</p><p>The other was shot on a 35mm camera. My mom and dad stand next to their best man and maid of honor, but the details are lost because the exposure is far too dark. And the negative was lost long ago too.</p><p>These are two photos that are important to my history, but I’ll probably never be able to make them any better. One was shot on a lost format, the color and contrast embedded into a single original copy as it was shot. The other could be cleaned up and blown up significantly with modern technology — if only the negative had been handled with care.</p><p>Everyone has images that are precious to them: that miraculous video of your dog doing that trick he does. The shot of your grandparents on their final anniversary together. Your own wedding, in which you invested thousands of your own savings. Imagine if you couldn’t play the video anymore, or if your grandparents’ portrait got blurry, or if your skin and wedding dress had a green hue on it.</p><p>On a movie or television show, this type of thing happens all the time to the director or cinematographer. They have worked their entire lives to get to the point of capturing that picture, and capturing it correctly. And then later it looks bad and wrong — 24 times a second.</p><p>By reaching beyond film and television art and into the realms of history and science, we’ve been working on solving this issue for Netflix projects. The future of our industry always has unknowns thanks in large part to rapidly accelerating innovations in technology. However, we can use what we <em>do</em> know from a hundred years of filmmaking, and the study of human perception, to best preserve content as new technology emerges which allows us to make the experience of viewing it even better. Preserving creative intent while preserving these important images is the goal.</p><p>With some attention and care spent up front on building and testing a color managed workflow, a show can look as expected at every point in the process, and archival assets can be created that increase the longevity of a show far into the future and protect it for remastering. The assets we require at Netflix — the NAM, GAM, and VDM — might be digital files that make their way to the cloud via our Content Hub, but the concepts are rooted in history and science.</p><h3>What’s in a NAM, GAM or VDM?</h3><p>Anyone who has delivered to (or been interested in delivering to) Netflix is familiar with these terms: <strong>non-graded archival master</strong> (NAM), <strong>graded archival master</strong> (GAM), and <strong>video display master</strong> (VDM). Other studios or facilities may have similar assets or similar names, and these are ours. Generally speaking, each delivery to Netflix will include these archival assets.</p><p>A <strong>non-graded archival master</strong> (NAM) is a complete copy of the ungraded, yet fully conformed, final locked picture including VFX, rendered in the original working color space such as ACES or the original camera log space, with no output or display transform rendered in or applied.</p><p>A <strong>graded archival master</strong> (GAM) adds in the final color grading decisions to the fully conformed final locked picture, and is also rendered in the original working color space such as ACES or the original camera log space — <em>again</em>, with no output or display transform rendered in or applied.</p><p>Neither the NAM nor GAM are very pretty to watch because images in logarithmic a.k.a “log” or linear space are not rendered for display and potentially hold more information than displays can reproduce. For that, we have the VDM.</p><p>The <strong>video display master</strong> (VDM) is <em>also</em> a complete copy of the fully conformed, final locked picture with VFX, this time <strong>rendering in the output or display transform</strong>, meaning it is encoded in the mastering display’s color space.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zU61sFIsT-nEL6ZSC8dTug.png" /><figcaption><em>NAM-GAM-VDM Example: Log workflow (ARRI LogC)</em></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Kj4fUnPWtyO44dVTcfhrxg.png" /><figcaption><em>NAM-GAM-VDM Example: Linear workflow (ACES)</em></figcaption></figure><p>Each of these assets is delivered in an uncompressed or losslessly compressed format such as a 16-bit DPX, EXR, or TIFF sequence. These <em>archival</em> assets are among a variety that are required in addition to delivery of a SMPTE IMF (Interoperable Master Format) package, which is the mastering format used to derive all streaming assets.</p><p>These assets give us a huge amount of flexibility in the future because we’re preserving a copy of the show in the original color space. While retaining all the original information and dynamic range as it was originally shot, we can remaster shows (and remaster them more easily) while preserving the original creative intent, assuring they’ll continue to look the best they possibly can for years to come.</p><p>To understand where these terms and processes come from, we have to go back to film class.</p><h3>Film History 101</h3><p>Thinking more deeply about Netflix and the technology evolution pushing forward the creative technical work behind television and film, the last thing to come to mind might be physical film. But in fact, our NAM, GAM and VDM archival assets have their roots in over a hundred years of film history.</p><p>Today, more often than not, productions have largely moved to digital acquisition on camera cards and hard drives. A century ago, when the only image capture format was celluloid, the physical processes to handle it were developed and refined over the years that followed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4IXIhJ8sN1v0aJa96kmNUQ.jpeg" /><figcaption>A motion picture film strip (Source: <a href="https://en.wikipedia.org/wiki/Film_stock#/media/File:Film_strip.jpg">Wikimedia Commons</a>)</figcaption></figure><p>In a physical film workflow, photography was completed on set and exposed film negatives were sent to a lab for a one light print. Multiple camera rolls were strung together into a lab roll, and dailies were created using a simple set of light values that made a positive human-viewable print.</p><p>An editor would cut together the film and a negative cut list (similar to an EDL, with a list of edit decisions and key codes instead of file names and time codes) was sent to a negative cutter for conforming the locked picture from the original negative.</p><p><strong>This final cut glued together by a negative cutter is the equivalent of our modern day non-graded archival master (NAM).</strong></p><p>After this point, a director of photography would work with a color timer to apply a one-light to the entire negative, then give creative adjustments on each scene. The color timer would program the printer lights on a shot by shot basis in an analog process, creating what would be similar to a modern color decision list (CDL). When the color was agreed upon, the negative was printed with the timing lights. This second negative — a negative of a negative — was called the interpositive (IP).</p><p><strong>This IP or “negative negative” with all the final color decisions included is the equivalent of our graded archival master (GAM). </strong>Since this film stock is based on the original negative, it can hold the same amount of information and dynamic range as the original negative.</p><p>Internegatives were created from the IP for bulk printing, and a positive (human viewable) film print was created from that. A print film stock, unlike negative film stocks, has specific characteristics required to produce a pleasing image when shown on a film projector. <strong>The film print is our equivalent of a video display master (VDM).</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/630/1*_Gw4yNRRtQeiSPHWQnNMrw.jpeg" /><figcaption>35mm film print (Image courtesy of Adakin Productions) Source: <a href="https://commons.wikimedia.org/wiki/File:Anamorphic-digital_sound.jpg">Wikipedia</a></figcaption></figure><p>As film continued to evolve and transition into digital workflows, motion imaging experts have continued to innovate and improve this process and transition it into modern workflows. Decades of work on moving pictures combined with the proliferation of faster, cheaper storage and smaller, better camera sensors have led to the ability to create a robust archive ready for remastering where no scenes will ever be lost to time.</p><h3>Next Up, Science Class: Color Science</h3><p>To create these archival assets in today’s digital workflows (and maintain a happy creative team viewing their show exactly as they shot it at every point in the process), proper color management is key from the start. A basic understanding of color science is helpful in understanding how and why color, perception, and display technology is critical.</p><p>Most pictures today are color pictures. Color is made up of light, which at different wavelengths we would call “red,” “green,” “blue,” and many other names.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tsvPFgdz1zqkgMZM-WLRFw.png" /><figcaption>Source: <a href="https://www.colour-science.org">Colour Science for Python</a></figcaption></figure><p>This light goes through two phases:</p><ol><li>Light enters the eye and tiny cells on our retina (cones) react to it.</li><li>Signal travels to the back of our brain (visual cortex) to form a color perception.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/820/1*4cAnXDPEXS3Jttf6iWz3yg.jpeg" /><figcaption>Source: <a href="https://commons.wikimedia.org/wiki/File:Seeing_red.jpg">Wikimedia Commons</a></figcaption></figure><p>The eye-retina portion (1) is a fairly well-understood science, standardized by the CIE into three measurable “tristimulus values” known as XYZ.* These values are based on our three cone types which respond to long (L), medium (M), and short (S) wavelengths of light.</p><p>XYZ is often called colorimetry, or the measurement of color. If you can get XYZ₁ to match XYZ₂, to the average observer, the colors will match. For example, we can print a picture of an apple, using printer dyes, which measures the same XYZ values as the original apple, even though the exact spectral characteristics of the printer dyes and apple differ. This is how most color imaging systems are successful.</p><p>The cognitive portion (2) is far more complex, and involves your viewing environment, adaptation state, as well as expectations and memory. This is known as color appearance, and is also well-studied and modeled — but we’ll save that for a future blog.</p><p>For this reason, XYZ makes for a fine and proven way of calibrating displays to match. Until someone figures out how to feed content straight to your brain, displays are the only way we can view content, so understanding their characteristics and making sure they’re working as intended is important.</p><p>But before we get to the display, we have to <em>create</em> the images to display.</p><p>Cameras in our industry typically attempt to respond to light as close to the human visual system as possible, using color filters to emulate the three cone responses of the human eye. A perfectly designed camera would be able to record all visible colors, store them in XYZ, and perfectly store all the colors of a scene! Unfortunately, achieving this with an electronic camera system is difficult, so most cameras are not perfectly colorimetric. Still, the “emulate-the-human-eye” design criteria remains, and most cameras do a fairly good job at it.</p><p>Since cameras are not perfect, in very simple terms, they do two things:</p><ol><li>Apply a Input Transform from raw sensor RGB → XYZ colorimetry, optimizing this transform for the most important** colors found in the real world.</li><li>Apply an Output Transform from XYZ → RGB for display.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bfg3Bq3ziOTb1modc1PMQg.png" /></figure><p>Sometimes this is all done in one step. For example, when you get a JPEG out of a camera or your smartphone, these two steps have occurred, and you are looking at what is known as a “display-referred” image. In other words, the RGB values correspond to the colors that should be coming off of the display.</p><p>It is worth noting here that broadcast cameras often operate in the same manner — they apply #1 and #2 to output “display-referred” images, which can be sent directly to a display.</p><p>Shooting RAW is different. Professional cameras allow for #1 and #2 to <em>not</em> be applied. This means you get the raw sensor RGB values. No color transforms are applied until you process or “develop” that image.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/988/1*YSQn0pAMEhCqxRYQJE71pg.png" /></figure><p>Now, let’s say you applied color transform #1 from above, but not #2, and you output an image in XYZ. This is known as a “scene-referred” image. In other words, the values correspond to the estimated*** colors in the scene, either in XYZ or an RGB encoding defined within XYZ.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4qM0kipHVocqczRQBIRmww.png" /></figure><p>Scene-referred images usually contain much more information than displays can show, just like a film negative. This is true in both dynamic range and color. This can be stored in various ways. Camera companies in our industry usually define their own “scene-referred” color spaces.</p><p>Just a few examples include:</p><ul><li>ARRI: Alexa LogC Wide Gamut</li><li>Sony: S-Log3 S-Gamut3.cine</li><li>Panasonic: V-Log V-Gamut</li><li>RED: RED Wide Gamut Log3G10</li></ul><p>These color spaces are designed specifically to encompass the range of light and color that each camera is capable of capturing, and are storable in an integer encoding (usually 10-bit or 12-bit). This takes care of the Input Transform (#1).</p><p>It might seem appropriate to simply show the scene-referred color on a display, but the Output Transform (#2) portion is required to account for differences in luminance between the scene and display, as well as the change in viewing environment. For example, a <em>picture</em> of a sunny day is not nearly as bright as the physical sun, so this must be accounted for in terms of contrast and color. This concept of “picture rendering” has many approaches, and goes beyond the scope of this blog post, but since it has a large impact on the overall “look” of an imaging system, it is worth introducing the concept here.</p><p>For this reason, camera companies usually provide default Output Transforms (in the form of look-up tables or LUTs) so that you can take a Log image from their camera and view it in a color space such as BT. 1886.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ke4FVSqOjupFb7J5vMlzRQ.png" /><figcaption>Source: <a href="https://www.kodak.com/US/en/motion/support/technical_information/lab_tools_and_techniques/digital_lad/default.htm">Kodak</a></figcaption></figure><h3>An Eye Toward Color Management</h3><p>These concepts all come together to form a color managed workflow. Because color management assures image fidelity, predictability of viewing images, and ease with mixed image sources, it’s the best way to work to protect the present <em>and</em> future viewing of movies or series. A color managed workflow requires a defined working color space and an agreed upon output transform or LUT, clearly documented and shared with all parties in a workflow.</p><p>Once a working color space is defined, all color corrections are made within that space. However, since we know this space is scene-referred and can’t be viewed directly, the output transform must be used to preview what the image will look like on your display.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ahsmGoskvuIm3bUsPdjvvw.png" /><figcaption><em>In this example, the working color space is Log and the display color space is BT. 1886. The Output Transform separates the two, and is only baked in for the BT. 1886 streaming master. The archival masters (non-graded and graded) remain in Log color space.</em></figcaption></figure><p>It might <em>seem</em> easier to just convert all images to a display color space like BT. 1886. But this removes all the dynamic range and additional information from the post production process and the resulting archive. As new display technology emerges as years pass by, your images are stuck in time like Kylee’s parents’ wedding photos.</p><p>Using an Output Transform or display LUT — such as a creative LUT designed by a colorist or DI facility, or even a default camera LUT like ARRI’s 709 LUT — can not only serve as the base “look” of the show, but it protects and preserves the working color space and the full dynamic range it has to offer for color and VFX, and the eventual NAM and GAM archival assets.</p><p>Additionally, in productions with secondary cameras, an Input Transform can be used to convert images into this larger working color space. Most professional cameras have published color space definitions, and most professional color grading software implement these in their toolsets. This unifies images into a common color space and reduces time spent matching different cameras to one another.</p><p>The Academy Color Encoding Standard (ACES) is a color management system which attempts to unify these “scene-referred” color spaces into a larger, standardized one. It covers all visible colors, and uses 16-bit half-float (32 stops of linear dynamic range) encoding stored in an <a href="http://www.openexr.com/">OpenEXR</a> container, well beyond the range of any camera today. Camera manufacturers also publish Input Transforms in order to convert from their native sensor RGB into ACES RGB.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*an1RsgBwHuN3e_UWt635YA.png" /><figcaption>Source: <a href="https://acescentral.com/">Academy of Motion Picture Arts and Sciences</a></figcaption></figure><p>ACES also defines a standard set of Output Transforms in order to provide a standard way to view an image on a calibrated display, regardless of the camera. This part is key in terms of providing a consistent view of working images, since these ACES Output Transforms are built-in to most popular color grading and VFX software.</p><p>It’s worth noting that in the past, color transforms had to be exported into fixed LUTs (look-up tables) for performance reasons. However, increasingly with modern GPUs, systems are able to apply the pure math of a color transform without the need for LUTs.</p><h3>But what about viewing it all?</h3><p>Similar to camera color spaces, display color spaces are typically defined in XYZ space. However, no current displays can properly show everything in most scene-referred images due to absolute luminance and color gamut limitations, and the evolution of display technology means what you can see will change year by year.</p><p>Displays receive a signal and output light. Display standards, and calibration to those standards, allow us to send a signal and get a predictable output of light and color.</p><p>Today, most displays are additive, in that they have three “primaries” which emit red, green, and blue (RGB) light, and when combined or added, they form white. The ‘white point’ is the color that is produced when equal amounts of red, green, and blue are sent to the monitor.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/300/1*ZnnUPWxb9459dPhvmatdmQ.png" /><figcaption>Source: <a href="https://commons.wikimedia.org/wiki/File:AdditiveColor.svg">Wikimedia Commons</a></figcaption></figure><p>Display standards exist so that you can take an image from Display #1 and send it to Display #2 and get the same color. In other words, <em>which </em>red, green, blue, and white are being used?</p><p>This is especially important in broadcast TV and the internet, where images are sent to millions of displays simultaneously. Common display standards include sRGB (internet, mobile), BT. 1886 (HD broadcast), Rec. 2020 (UHD and HDR broadcast), and P3 (digital cinema and HDR).</p><p>These standards define three main components:</p><ul><li>Primaries, usually defined in XYZ</li><li>White point, usually defined in XYZ</li><li>EOTF (Electro-Optical Transfer Function / signal-to-luminance (Y))</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/858/1*a4ZCt0Mr0EXjg-J9DYXO7g.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/880/1*1AYzw3yJmwiLe1FE0U3kTA.png" /><figcaption>Example Primaries and White Point of ITU-R BT.709 aka “Rec. 709” color space (left), Example Transfer Function from ITU-R BT.1886 standard (right)</figcaption></figure><p>Test patterns are measured in order to tune displays to hit standard targets as closely as possible. Usually the test patterns include patches of red, green, blue, and white, as well as a greyscale to measure the EOTF.</p><p>Only after calibration do creative adjustments become meaningful, and color transforms become useful. A color managed workflow requires this step in order to truly have an impact on image fidelity and consistency.</p><p>Display technology has come a long way since we first began exhibiting motion images in public and then the home. From projection systems in theaters to over-the-air broadcast to emissive displays like OLED and even an iPhone, the display technology on which we look at images continues to evolve quickly. Color management and proper archival elements protect the high quality display of a movie for the future.</p><h3>Summary</h3><p>Returning to the original story of my parents and their remaining wedding photos, it’s apparent how being in the moment and dealing with all the challenges of that moment — budget, time, people, access to technology — can seem small compared to what is happening around you. But as time goes on, the remaining records of that experience only become more precious. And the opportunity to preserve it — and preserve it well — is missed altogether.</p><p>Drawing a parallel to a film or television show, preserving these captured moments and assuring the creative intent behind them is preserved and protected for years of enjoyment is incredibly important. Some shows become cultural touchstones. Others are personal favorites that provide comfort for many years. In any case, for many filmmakers they are the culmination of an individual’s life work and deserve the respect of a high quality viewing experience and high fidelity archive.</p><p>At Netflix, we are constantly refining our process and approach to these processes while continuing to rely on the vast collective knowledge of so many years of film history and scientific research. Within the Creative Technologies &amp; Infrastructure team, we are always on the hunt for new and innovative ways to increase the usefulness of assets while becoming more and more flexible for creatives and technicians alike. While history and science may give us many resources to work from, the relationships we cultivate with the production community may give the best guidance.</p><p>Some movies have been lost to time. Some remastered films are missing entire scenes. And I’m not the only one whose family photo albums are rapidly fading, their quality locked in by the imaging technology available at the time. By combining thoughtful planning and technology, we can preserve the human experience and its stories for decades to come.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sES8ck5qDUkty99RZvMRPQ.png" /><figcaption>Despite improperly captured and archived wedding photos, Kylee’s parents are still happily married after 40 years. And this photo was captured in Canon RAW and is backed up in two locations.</figcaption></figure><h4>Footnotes</h4><p><em>*XYZ, in this blog, refers to the CIE 1931 2-degree color matching functions (CMFs). This is different than DCI X’Y’Z’ (pronounced “X-prime, Y-prime, Z-prime”) which is a color encoding, based on XYZ, defined for digital cinema.</em></p><p><em>**Examples might be skin tones, blue skies, foliage, and other common colors.</em></p><p><em>***The word ‘estimated’ is used since cameras are not perfect colorimetric devices.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e21a9fb54988" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/protecting-a-storys-future-with-history-and-science-e21a9fb54988">Protecting a Story’s Future with History and Science</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>